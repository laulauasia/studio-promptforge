// This is an autogenerated file from Firebase Studio.

'use server';

/**
 * @fileOverview Detects harmful content in a given prompt.
 *
 * - detectHarmfulness - A function that detects harmful content in a prompt.
 * - DetectHarmfulnessInput - The input type for the detectHarmfulness function.
 * - DetectHarmfulnessOutput - The return type for the detectHarmfulness function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const DetectHarmfulnessInputSchema = z.object({
  prompt: z.string().describe('The prompt to check for harmful content.'),
});
export type DetectHarmfulnessInput = z.infer<typeof DetectHarmfulnessInputSchema>;

const DetectHarmfulnessOutputSchema = z.object({
  isHarmful: z.boolean().describe('Whether the prompt contains harmful content.'),
  reason: z.string().describe('The reason why the prompt is considered harmful, if applicable.'),
});
export type DetectHarmfulnessOutput = z.infer<typeof DetectHarmfulnessOutputSchema>;

export async function detectHarmfulness(input: DetectHarmfulnessInput): Promise<DetectHarmfulnessOutput> {
  return detectHarmfulnessFlow(input);
}

const detectHarmfulnessPrompt = ai.definePrompt({
  name: 'detectHarmfulnessPrompt',
  input: {schema: DetectHarmfulnessInputSchema},
  output: {schema: DetectHarmfulnessOutputSchema},
  prompt: `You are an AI safety expert. Your job is to review the given prompt and determine if it contains any harmful content, such as hate speech, dangerous content, harassment, or sexually explicit material.

  Prompt: {{{prompt}}}

  Respond with whether the prompt is harmful and the reason for it.
  If the prompt is not harmful, the isHarmful value should be false, and the reason should be 'Not harmful'.`,
  config: {
    safetySettings: [
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_ONLY_HIGH',
      },
      {
        category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
        threshold: 'BLOCK_NONE',
      },
      {
        category: 'HARM_CATEGORY_HARASSMENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
        threshold: 'BLOCK_LOW_AND_ABOVE',
      },
    ],
  },
});

const detectHarmfulnessFlow = ai.defineFlow(
  {
    name: 'detectHarmfulnessFlow',
    inputSchema: DetectHarmfulnessInputSchema,
    outputSchema: DetectHarmfulnessOutputSchema,
  },
  async input => {
    const {output} = await detectHarmfulnessPrompt(input);
    return output!;
  }
);
